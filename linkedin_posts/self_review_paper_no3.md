ğŸ” ğ—¦ğ—²ğ—¹ğ—³-ğ—¥ğ—²ğ˜ƒğ—¶ğ—²ğ˜„ğ—¶ğ—»ğ—´ ğ— ğ˜† ğ—¢ğ˜„ğ—» ğ—¥ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µ: ğ—£ğ—®ğ—½ğ—²ğ—¿ #ğŸ¯



ğŸ‘‰ ğ—¥ğ—²ğ—®ğ—± ğ˜ğ—µğ—² ğ—œğ—»ğ˜ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—£ğ—¼ğ˜€ğ˜ ğ—µğ—²ğ—¿ğ—²: https://lnkd.in/eS_FjQHF



âœ… ğ—£ğ—®ğ—½ğ—²ğ—¿: Range Entropy: A Bridge between Signal Complexity and Self-Similarity

ğ—ğ—¼ğ˜‚ğ—¿ğ—»ğ—®ğ—¹: Entropy, 2018





âœ… ğ—¥ğ—²ğ˜ƒğ—¶ğ—²ğ˜„ğ—²ğ—¿ #ğŸ® ğ—–ğ—¼ğ—ºğ—ºğ—²ğ—»ğ˜ğ˜€



1. ğ—¦ğ—µğ—®ğ—¸ğ˜† ğ—±ğ—²ğ—³ğ—¶ğ—»ğ—¶ğ˜ğ—¶ğ—¼ğ—»: The new â€œrange distanceâ€ is not really a proper distance measure. It may show smaller values even when signals drift further apart. That undermines the whole mathematical foundation.



2. ğ—•ğ˜‚ğ—¶ğ—¹ğ˜-ğ—¶ğ—» ğ—°ğ—¼ğ—¹ğ—¹ğ—®ğ—½ğ˜€ğ—²: By design, RangeEnA/B always goes to zero when the tolerance r is set to 1. This makes results look â€œbounded and stable,â€ but also strips away useful differences across signals.



3. ğ—¨ğ—»ğ—±ğ—²ğ—³ğ—¶ğ—»ğ—²ğ—± ğ˜ƒğ—®ğ—¹ğ˜‚ğ—²ğ˜€: For small tolerance values, the measure often hits â€œlog of zero,â€ producing missing results. Instead of a clear fix, the paper just glosses over this problem.



4. ğ—¢ğ˜ƒğ—²ğ—¿ğ˜€ğ˜ğ—®ğ˜ğ—²ğ—± ğ—¹ğ—¶ğ—»ğ—²ğ—®ğ—¿ğ—¶ğ˜ğ˜†: The supposed â€œlinearâ€ link between Range Entropy and the Hurst exponent (a measure of self-similarity) is eyeballed from plots, without confidence intervals or error analysis. 



5. ğ—™ğ—¹ğ—®ğ˜„ğ—²ğ—± ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ: The Hurst exponent itself is estimated using a biased old method (R/S). For short signals, this method is unreliable, so the nice-looking correlations may be misleading.



6. ğ—¡ğ—®ğ—¿ğ—¿ğ—¼ğ˜„ ğ˜ğ—²ğ˜€ğ˜ğ—¶ğ—»ğ—´: Only a limited set of simulated signals were explored. No sweep across heavy-tailed noise or broader parameter ranges, so generality is claimed without evidence.



7. â€œğ—¥ğ—¼ğ—¯ğ˜‚ğ˜€ğ˜ğ—»ğ—²ğ˜€ğ˜€â€ ğ—¼ğ˜ƒğ—²ğ—¿ğ˜€ğ—¼ğ—¹ğ—±: The paper highlights stability under amplitude scaling but ignores simple additive drifts that can actually break the measure. Robust? Only partly.



8. ğ—¡ğ—¼ ğ˜ğ—µğ—²ğ—¼ğ—¿ğ˜†: There is no mathematical analysis of the statistical properties such as bias and variance. Everything rests on simulations and plots.



9. ğ—˜ğ—˜ğ—š ğ—±ğ—²ğ—ºğ—¼ ğ—¶ğ˜€ ğ—»ğ—¼ğ˜ ğ—½ğ—¿ğ—¼ğ—¼ğ—³: The Bonn epilepsy dataset is tiny and cherry-picked. The paper shows separation between groups but without effect sizes, statistical tests, or predictive validation.



âœ… ğ—¢ğ˜ƒğ—²ğ—¿ğ—®ğ—¹ğ—¹ ğ—”ğ˜€ğ˜€ğ—²ğ˜€ğ˜€ğ—ºğ—²ğ—»ğ˜



Range Entropy is a clever idea that makes complexity curves look neat and bounded. But the math is shaky, the validation thin, and the clinical demo too weak. At best itâ€™s an interesting prototype, not a proven tool.



âœ… ğ—ªğ—µğ—®ğ˜ ğ—œ ğ˜„ğ—¼ğ˜‚ğ—¹ğ—± ğ—±ğ—¼ ğ˜ğ—¼ğ—±ğ—®ğ˜†:



â€¢ Redefine the distance so it behaves consistently.

â€¢ Provide theoretical proofs on stability and bias.

â€¢ Use stronger, unbiased methods for self-similarity.

â€¢ Test more broadly (more noise types, heavy tails).

â€¢ Add proper statistics and validation in EEG studies.



âœ… ğ—¢ğ˜ƒğ—²ğ—¿ğ—®ğ—¹ğ—¹ ğ—¾ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜†: Smart idea, but mathematically fragile and oversold.





#MedicalDataAnalysis #Neuroimaging #ResearchReflection #ScientificIntegrity #Reviewer2OnMyPapers
