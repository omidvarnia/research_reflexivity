🔍 𝗦𝗲𝗹𝗳-𝗥𝗲𝘃𝗶𝗲𝘄𝗶𝗻𝗴 𝗠𝘆 𝗢𝘄𝗻 𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵: 𝗣𝗮𝗽𝗲𝗿 #𝟯



👉 𝗥𝗲𝗮𝗱 𝘁𝗵𝗲 𝗜𝗻𝘁𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗣𝗼𝘀𝘁 𝗵𝗲𝗿𝗲: https://lnkd.in/eS_FjQHF



✅ 𝗣𝗮𝗽𝗲𝗿: Range Entropy: A Bridge between Signal Complexity and Self-Similarity

𝗝𝗼𝘂𝗿𝗻𝗮𝗹: Entropy, 2018





✅ 𝗥𝗲𝘃𝗶𝗲𝘄𝗲𝗿 #𝟮 𝗖𝗼𝗺𝗺𝗲𝗻𝘁𝘀



1. 𝗦𝗵𝗮𝗸𝘆 𝗱𝗲𝗳𝗶𝗻𝗶𝘁𝗶𝗼𝗻: The new “range distance” is not really a proper distance measure. It may show smaller values even when signals drift further apart. That undermines the whole mathematical foundation.



2. 𝗕𝘂𝗶𝗹𝘁-𝗶𝗻 𝗰𝗼𝗹𝗹𝗮𝗽𝘀𝗲: By design, RangeEnA/B always goes to zero when the tolerance r is set to 1. This makes results look “bounded and stable,” but also strips away useful differences across signals.



3. 𝗨𝗻𝗱𝗲𝗳𝗶𝗻𝗲𝗱 𝘃𝗮𝗹𝘂𝗲𝘀: For small tolerance values, the measure often hits “log of zero,” producing missing results. Instead of a clear fix, the paper just glosses over this problem.



4. 𝗢𝘃𝗲𝗿𝘀𝘁𝗮𝘁𝗲𝗱 𝗹𝗶𝗻𝗲𝗮𝗿𝗶𝘁𝘆: The supposed “linear” link between Range Entropy and the Hurst exponent (a measure of self-similarity) is eyeballed from plots, without confidence intervals or error analysis. 



5. 𝗙𝗹𝗮𝘄𝗲𝗱 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵: The Hurst exponent itself is estimated using a biased old method (R/S). For short signals, this method is unreliable, so the nice-looking correlations may be misleading.



6. 𝗡𝗮𝗿𝗿𝗼𝘄 𝘁𝗲𝘀𝘁𝗶𝗻𝗴: Only a limited set of simulated signals were explored. No sweep across heavy-tailed noise or broader parameter ranges, so generality is claimed without evidence.



7. “𝗥𝗼𝗯𝘂𝘀𝘁𝗻𝗲𝘀𝘀” 𝗼𝘃𝗲𝗿𝘀𝗼𝗹𝗱: The paper highlights stability under amplitude scaling but ignores simple additive drifts that can actually break the measure. Robust? Only partly.



8. 𝗡𝗼 𝘁𝗵𝗲𝗼𝗿𝘆: There is no mathematical analysis of the statistical properties such as bias and variance. Everything rests on simulations and plots.



9. 𝗘𝗘𝗚 𝗱𝗲𝗺𝗼 𝗶𝘀 𝗻𝗼𝘁 𝗽𝗿𝗼𝗼𝗳: The Bonn epilepsy dataset is tiny and cherry-picked. The paper shows separation between groups but without effect sizes, statistical tests, or predictive validation.



✅ 𝗢𝘃𝗲𝗿𝗮𝗹𝗹 𝗔𝘀𝘀𝗲𝘀𝘀𝗺𝗲𝗻𝘁



Range Entropy is a clever idea that makes complexity curves look neat and bounded. But the math is shaky, the validation thin, and the clinical demo too weak. At best it’s an interesting prototype, not a proven tool.



✅ 𝗪𝗵𝗮𝘁 𝗜 𝘄𝗼𝘂𝗹𝗱 𝗱𝗼 𝘁𝗼𝗱𝗮𝘆:



• Redefine the distance so it behaves consistently.

• Provide theoretical proofs on stability and bias.

• Use stronger, unbiased methods for self-similarity.

• Test more broadly (more noise types, heavy tails).

• Add proper statistics and validation in EEG studies.



✅ 𝗢𝘃𝗲𝗿𝗮𝗹𝗹 𝗾𝘂𝗮𝗹𝗶𝘁𝘆: Smart idea, but mathematically fragile and oversold.





#MedicalDataAnalysis #Neuroimaging #ResearchReflection #ScientificIntegrity #Reviewer2OnMyPapers
